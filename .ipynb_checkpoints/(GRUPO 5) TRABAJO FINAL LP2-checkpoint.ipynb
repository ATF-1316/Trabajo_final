{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando web scraping a la página UniProt\n",
    "\n",
    "## Presentado por:\n",
    "- Campuzano Galarza, Sofia Gabriela\n",
    "- Cordova Quispe, Brigitte Nayely\n",
    "- Tejada Flores, Antonella Franchesca\n",
    "\n",
    "## Objetivo\n",
    "Extraer información de la entrada (Entry), nombre de entrada (Entry name), proteína (Protein), gen (Gene), organismo (Organism), estado (Status) y GO-Proceso biológico (GO-Biological Process) de los más de seis mil genes abreviados (Gen Abrev) para posteriormente almacenarlos en un archivo excel.\n",
    "\n",
    "\n",
    "## ¿La página tiene un API?\n",
    "Tras una intensa busqueda, realizada por el grupo, pudimos validar que no existe API alguno para esta página. Por lo que  optamos por inspeccionar de manera manual el html de la página y luego con ayuda del paquete BeautifulSoup pudimos extraer la data solicitada en el archivo Excel_UniProt_Vacio.xlsx.\n",
    "\n",
    "### Nota:\n",
    "- Se recomienda guardar el archivo ipynb y el excel vacío en la misma carpeta para que no haya problemas al ejecutar el código. \n",
    "\n",
    "- Este código está trabajando con un archivo excel de nombre \"02 Ch Genes ordenados con vecinos_PROTOTIPO.xlsx\", ya que contiene una muestra de 16 genes. Si se desea correr todos los genes que existen en la página de **UniProt.org**, cambie el nombre del archivo excel por el de \"Excel_UniProt_Vacio.xlsx\". Debe realizar los cambios en la línea 9, 135 y 146.\n",
    "\n",
    "- La recolección de datos se dio desde las 16:00 hasta las 00:25 (8 horas aproximadamente).\n",
    "\n",
    "### Link de la página:\n",
    "https://www.uniprot.org\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : TRNAS-GGA\n",
      "2 : LOC102173449\n",
      "3 : LOC102169333\n",
      "4 : LOC108636757\n",
      "5 : LOC102187712\n",
      "6 : VGLL3\n",
      "7 : CHMP2B\n",
      "8 : POU1F1\n",
      "9 : HTR1F\n",
      "10 : LOC102188976\n",
      "11 : CGGBP1\n",
      "12 : ZNF654\n",
      "13 : C1H3orf38\n",
      "14 : LOC102190548\n",
      "15 : SLC12A8\n",
      "16 : LOC106502043\n"
     ]
    }
   ],
   "source": [
    "                   ########################### PAQUETES A IMPORTAR ###########################\n",
    "import requests\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "                ################## CÓDIGO INPLEMENTADO PARA EL WEB SCRAPING  ######################\n",
    "    \n",
    "archivo_vacio = pd.read_excel(\"02 Ch Genes ordenados con vecinos_PROTOTIPO.xlsx\",header=1) # Lee el archivo excel que se tiene guardada en la misma carpeta del script.\n",
    "genes=list(archivo_vacio[\"Gen Abrev\"]) # De la variable archivo_vacio que guarda la información del excel,\n",
    "# se extre la columna \"Gen Abrev\" para su posterior uso en la iteración a la hora de buscar gen por gen.\n",
    "\n",
    "\n",
    "n=3 # variable que itera y ubica la fila de genes en el archivo excel.\n",
    "for i in genes: # Por cada elemento en la lista de genes:\n",
    "    print(n-2,\":\",i)\n",
    "    url=f\"https://www.uniprot.org/uniprot/?query={i}+Capra+Hircus&sort=score\" # Guarda la url que va iterando cada fila de los elementos de la columna \"Gen Abrev\". \n",
    "    page_response=requests.get(url) # Hace una \"solicitud\" a la página del tipo \"obtener\" para extraer la información\n",
    "    # de la dirección de url.\n",
    "    page_content=BeautifulSoup(page_response.content,\"html.parser\") # Analiza la respuesta del \"page.response\" y lo guarda como\n",
    "    # un html para que pueda recorrer por el código fuente.\n",
    "    \n",
    "    \n",
    "    # Con el fin de entrar a la página de cada gen para extraer la información solicitado,\n",
    "    # Se realiza lo siguiente:\n",
    "\n",
    "    entry=page_content.find_all(\"td\",class_=\"entryID\") # Encuentra todos las etiquetas  \"td\" con las clase \"entryID\".\n",
    "    entry_i=[] # Lista vacía para almacenar la información que esta guardada en la variable entry.\n",
    "    for e in entry:  # Por cada elemento en entry:\n",
    "        entry_i.append(e.text) # Añade a la lista vacía solo el texto que encuente \"entry\".\n",
    "        \n",
    "        \n",
    "    # Utilizamos \"try\" porque a la hora de iterar buscando página por página y no encuentre nada de información en un gen,\n",
    "    # se pase a la búsqueda de otro gen. De esta manera evitamos errores en la extracción de información. \n",
    "        \n",
    "    try: # Intenta: \n",
    "        url=f\"https://www.uniprot.org/uniprot/{entry_i[0]}\" # Guarda la url de la página de cada gen, que se itera por el texto extraído de la primera fila de \"entry_i\"\n",
    "        page_response=requests.get(url) # Realiza la \"solicitud\" a la página del tipo \"obtener\" para extraer la información\n",
    "        # de la dirección url.\n",
    "        page_content=BeautifulSoup(page_response.content,\"html.parser\") # Analiza la respuesta del \"page.response\" y\n",
    "        # lo guarda como un html para que pueda recorrer por el código fuente.\n",
    "            \n",
    "            \n",
    "        ####  ENTRY  ####\n",
    "        entry_name=page_content.find_all(\"h2\",class_=\"page-title\") # Busca todas la etiquetas \"h2\" acompañada de la clase \"page-title\"\n",
    "        for j in entry_name: # Para cada elemento en entry_name:\n",
    "            entry1=j.text # extrae el texto que se encuentre en los elementos buscados de entry_name.\n",
    "        entry1=entry1.split() # Se utiliza la función split para separa los elementos de la lista.\n",
    "        Entry=entry1[2] # Como la lista se cuenta a partir del 0, se selecciona el elemento 2 de la lista entry1.  \n",
    "        \n",
    "        \n",
    "        #### ENTRY NAME ####\n",
    "        Entry_name=\"\".join(list(entry1[3])[1:len(list(entry1[3]))-1])\n",
    "        # De la lista entry1 selecciona el elemento 3, dicho elemento lo pasa a una lista, de dicha lista nueva \n",
    "        # selecciona a partir de la posición (1) toda la longitud hasta la ubicación (-1). De esta manera se evita las paréntesis.  \n",
    "\n",
    "        ###########################################################################################\n",
    "        \n",
    "        #### PROTEIN ####\n",
    "        proteina=page_content.find_all(\"h1\",property=\"name\") # Busca todas la etiquetas \"h1\" acompañado del \n",
    "        # atributo property de la variable name.\n",
    "        for j in proteina: # Para cada elemento en proteina:\n",
    "            Protein=j.text # Extrae el texto que se encuentre en cada elemento de \"proteina\".\n",
    "\n",
    "            \n",
    "        #### GEN ####\n",
    "        gabrev=page_content.find_all(\"div\",class_='entry-overview-content',id=\"content-gene\") # Busca todas las etiquetas \n",
    "        # \"div\" de atributo class y id con las variables correspondientes.\n",
    "        for j in gabrev: # Para cada elemento en gabrev:\n",
    "            Gen_Abrev=j.text # Extrae el texto que se encuentre en cada elemento de gabrev.\n",
    "\n",
    "            \n",
    "       #### ORGANISMO ####\n",
    "        organism_gen=page_content.find_all(\"div\",class_='entry-overview-content',id=\"content-organism\")\n",
    "        # Lo que se está haciendo es buscar la etiqueta 'div', con los atributos 'class' e 'id' y con sus valores 'entry-overview-content'\n",
    "        # y 'content-organism', respectivamente, dentro del page_content. Almacenando todo esto en una variable organism_gen\n",
    "        for j in organism_gen:\n",
    "            ORGANISM_GEN=j.text\n",
    "        # Aquí se hace un for para poder obtener, unicamente, el texto dentro de organism_gen, pues eso es lo que se nos solicita\n",
    "\n",
    "        \n",
    "        #### STATUS ####\n",
    "        estado=page_content.find_all(\"a\",title=\"Unreviewed (TrEMBL)\", class_=\"icon-uniprot unreviewed-icon tooltipped\")\n",
    "        # Aquí volvemos a hacer lo mismo que en el código anterior\n",
    "        # En page_content buscamos la etiqueta 'a', con sus atributos 'title' y 'class' y con los valores 'Unreviewed (TrEMBL)' y \n",
    "        #'icon-uniprot unreviewed-icon tooltipped', respectivamente. Procediendo a almacenar el resultado en la variable estado\n",
    "        \n",
    "        for j in estado:\n",
    "            ESTADO_PREVIO=j.text\n",
    "        # Al igual quel for anterior, se usa para poder obtener el texto dentro de la variable estado, guardando esa información en\n",
    "        # una variable ESTADO_PREVIO\n",
    "        ESTADO=str(ESTADO_PREVIO)\n",
    "        # En esta linea estamos haciendo que la variable ESTADO_PREVIO cambie de tipo, y se convierta en una variable de tipo str\n",
    "    \n",
    "    except: # Permite manejar el error respecto de los genes que estan vacios o no son encontrados\n",
    "        Entry =\"\"\n",
    "        Entry_name =\"\"\n",
    "        Protein =\"\"\n",
    "        Gen_Abrev =\"\"\n",
    "        ORGANISM_GEN =\"\"\n",
    "        ESTADO =\"\"\n",
    "    ##########################################################################\n",
    "    \n",
    "    # En este caso, estamos haciendo un nuevo try para la variable GO_PROCESO, dado que cuando esta se encuentra en el primer try, se presenta un error. \n",
    "    # Aquellos genes que en la variable GO_PROCESO no presenten valor, no se les permite imprimir ninguna de las otras variables que sí tienen un valor.\n",
    "    # Por lo que se opto, por hacer un try independiente para esta variable. Siendo de este modo posible, que las filas que no presentan dicho atributo \n",
    "    # puedan ser impresas con total normalidad.\n",
    "    \n",
    "    # Este try es el mismo que el que ya habíamos colado en un incio\n",
    "    try:\n",
    "        url=f\"https://www.uniprot.org/uniprot/{entry_i[0]}\"\n",
    "        page=requests.get(url)\n",
    "        page_content=BeautifulSoup(page.content,\"html.parser\")\n",
    "       \n",
    "        \n",
    "        #### GO BIOLOGICAL PROCESS ####  \n",
    "        go_secuencia = page_content.find(\"ul\", attrs= {\"class\":\"noNumbering biological_process\"})\n",
    "        #Aquí vamos a buscar dentro de page_content la etiqueta 'ul' con su repectivo atributo 'page_content' y valor 'noNumbering biological_process'\n",
    "        #Almacenando la información encontrada en una variable go_secuencia\n",
    "        GO_PROCESO=[] #creamos una lisita vacia\n",
    "        for secuencia in go_secuencia:  \n",
    "            pro_biological_info = secuencia.find_all(\"a\", attrs={\"onclick\":\"window.ga('UniProt-Entry-View', 'click', 'Display-GO-Term');\"})\n",
    "        #Este for, lo usaremos par apoder extraer, de go_secuencia, todo aquello que se encuentre bajo la etiqueta 'a', el tributo 'onclick' y con valor \n",
    "        #'window.ga('UniProt-Entry-View', 'click', 'Display-GO-Term');'. Procediendo a alamacenar lo obtenido en la variable pro_biological_info\n",
    "            for j in range(len(pro_biological_info)):\n",
    "                GO_PROCESO.append(pro_biological_info[j].text)\n",
    "        #El segundo for, subyugado al primero, se usará para obtener el texto que hay dentro del pro_biological_info, adicionando esa información \n",
    "        #a la variable GO_PROCESO\n",
    "        GO_PROCESO=str(GO_PROCESO)\n",
    "        #En esta linea estamos haciendo que la variable GO_PROCESO cambie de tipo, y se convierta en una variable de tipo str\n",
    "        \n",
    "    except: #Permite manejar el error respecto de los genes que estan vacios o no son encontrados\n",
    "        GO_PROCESO=\"\"\n",
    "        \n",
    "    excel=openpyxl.load_workbook(\"02 Ch Genes ordenados con vecinos_PROTOTIPO.xlsx\") \n",
    "    #Se abre el libro de trabajo del archivo excel\n",
    "    sheet=excel['Hoja1']\n",
    "    #De la hoja Hoja1\n",
    "    sheet.cell(row=n,column=2).value = Entry # Se accede al valor de la celda de la fila n y columna 2\n",
    "    sheet.cell(row=n,column=3).value = Entry_name # Se accede al valor de la celda de la fila n y columna 3\n",
    "    sheet.cell(row=n,column=4).value = Protein # Se accede al valor de la celda de la fila n y columna 4\n",
    "    sheet.cell(row=n,column=5).value = Gen_Abrev # Se accede al valor de la celda de la fila n y columna 5\n",
    "    sheet.cell(row=n,column=6).value = ORGANISM_GEN # Se accede al valor de la celda de la fila n y columna 6\n",
    "    sheet.cell(row=n,column=7).value = ESTADO # Se accede al valor de la celda de la fila n y columna 7\n",
    "    sheet.cell(row=n,column=8).value = GO_PROCESO # Se accede al valor de la celda de la fila n y columna 8\n",
    "    excel.save(\"02 Ch Genes ordenados con vecinos_PROTOTIPO.xlsx\") # Se guarda en el archivo excel \n",
    "    n=n+1 # Se llenan los datos hasta n+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
